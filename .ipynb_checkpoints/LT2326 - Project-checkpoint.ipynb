{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa1de23e",
   "metadata": {},
   "source": [
    "                                                                                            gusgraupa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc7a2f4",
   "metadata": {},
   "source": [
    "# LT2326 - Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd4f8a0",
   "metadata": {},
   "source": [
    "## Winograd Schema Challenge for Spanish: sentence probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e54c27fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchtext\n",
      "  Downloading torchtext-0.11.0-cp39-cp39-manylinux1_x86_64.whl (8.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 8.0 MB 4.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.9/site-packages (from torchtext) (4.62.0)\n",
      "Requirement already satisfied: requests in /usr/lib/python3.9/site-packages (from torchtext) (2.25.1)\n",
      "Collecting torch==1.10.0\n",
      "  Downloading torch-1.10.0-cp39-cp39-manylinux1_x86_64.whl (881.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 881.9 MB 590 bytes/s a 0:00:01    |█                               | 29.9 MB 14.7 MB/s eta 0:00:58\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib64/python3.9/site-packages (from torchtext) (1.19.5)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/site-packages (from torch==1.10.0->torchtext) (3.7.4.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/lib/python3.9/site-packages (from requests->torchtext) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/lib/python3.9/site-packages (from requests->torchtext) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3.9/site-packages (from requests->torchtext) (1.25.10)\n",
      "Installing collected packages: torch, torchtext\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.10.0+cu111 requires torch==1.9.0, but you have torch 1.10.0 which is incompatible.\n",
      "torchaudio 0.9.0 requires torch==1.9.0, but you have torch 1.10.0 which is incompatible.\u001b[0m\n",
      "Successfully installed torch-1.10.0 torchtext-0.11.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --user torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ea34969",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchtext.legacy.data import Field, BucketIterator, TabularDataset\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c08dc802",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04ceedb",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c448df",
   "metadata": {},
   "source": [
    "### Preparing Corpus sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d80f92",
   "metadata": {},
   "source": [
    "The models have been trained with two different datasets: one that comes from the Wikipedia and one that comes from Spanish books in Project Guthenberg.\n",
    "- For the first dataset, run processing_wiki_data.py to get \"wiki_sent.csv\".\n",
    "    - Then, in the console, I did tail -100000 wiki_sent.csv > medium_wiki_sent.csv\n",
    "- For the second dataset, run the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0dc483f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "# sent_tokenize(s)\n",
    "\n",
    "f = open('data/prova.txt', 'r')\n",
    "l = f.readlines()\n",
    "f.close()\n",
    "\n",
    "endtokens = '.?!:'\n",
    "\n",
    "g = open('sentences.csv', 'w')\n",
    "s = ''\n",
    "for line in l:\n",
    "    if line[-2] in endtokens:\n",
    "        s += line[:-1] + '\\n'\n",
    "        g.write(s)\n",
    "        s = ''\n",
    "    else:\n",
    "        s+= line[:-1]\n",
    "g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dad84e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(wiki_path):\n",
    "    whitespacer = lambda x: x.split(' ')\n",
    "    \n",
    "    WORDS = Field(tokenize    = whitespacer,\n",
    "                lower       = True,\n",
    "                batch_first = True,\n",
    "                init_token='<start>',\n",
    "                eos_token='<end>') \n",
    "    \n",
    "    # read the csv file\n",
    "    wikipedia = TabularDataset(path = wiki_path, # wiki_sent.csv\n",
    "                            format = 'csv',\n",
    "                            fields = [('sentence', WORDS)],\n",
    "                            skip_header       = True,\n",
    "                            csv_reader_params = {'quotechar':'Ö'}) \n",
    "    \n",
    "    # build vocabularies based on what our csv files contained and create word2id mapping\n",
    "    WORDS.build_vocab(wikipedia)\n",
    "\n",
    "    # create batches from our data, and shuffle them for each epoch\n",
    "    wikipedia_iter = BucketIterator(wikipedia,\n",
    "                                  batch_size        = 8,\n",
    "                                  sort_within_batch = True,\n",
    "                                  sort_key          = lambda x: len(x.sentence),\n",
    "                                  shuffle           = True,\n",
    "                                  device            = device)\n",
    "\n",
    "    return wikipedia_iter, WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8587efd",
   "metadata": {},
   "source": [
    "### Preparing WCS sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f756d471",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_un = '/home/gusgraupa@GU.GU.SE/MLNLP2/LT2326-Project/data/Dataset_with_pn_undescores_det.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cd9801d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_creation(filename):\n",
    "    f = open(filename, 'r')\n",
    "    l = f.readlines()\n",
    "    \n",
    "    grouped = []\n",
    "    j = []\n",
    "    for e in l:\n",
    "        if e == '\\n':\n",
    "            grouped.append(j)\n",
    "            j = []\n",
    "        else:\n",
    "            j.append(e[:-1])\n",
    "            \n",
    "    sentences = []\n",
    "    \n",
    "    for group in grouped:\n",
    "        \n",
    "        if group[1] == 'su' or group[1] == 'sus':\n",
    "            sentence = group[0][:-1]\n",
    "            words = sentence.split()\n",
    "            for i, w in enumerate(words):\n",
    "                if w == '_' + group[1] + '_': \n",
    "                    words.remove(w)\n",
    "                    words_copy = words.copy()\n",
    "                    words.insert(i+1, 'de ' + group[2])\n",
    "                    words_copy.insert(i+1, 'de ' + group[3])\n",
    "                    \n",
    "                    s1 = ' '.join(words) + '.'\n",
    "                    s2 = ' '.join(words_copy) + '.'\n",
    "                    \n",
    "                    g1 = (s1.lower(), group[2] == group[4])\n",
    "                    g2 = (s2.lower(), group[3] == group[4])\n",
    "        \n",
    "        else:\n",
    "            check = '_' + group[1] + '_'\n",
    "            \n",
    "            s1 = group[0].replace(check, group[2])\n",
    "            s2 = group[0].replace(check, group[3])\n",
    "            \n",
    "            g1 = (s1.lower(), group[2] == group[4])\n",
    "            g2 = (s2.lower(), group[3] == group[4])\n",
    "        sentences.append((g1, g2))\n",
    "        \n",
    "    f.close()\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acf3e668",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('los concejales de la ciudad denegaron el permiso a los manifestantes porque los concejales de la ciudad temían la violencia.',\n",
       "  True),\n",
       " ('los concejales de la ciudad denegaron el permiso a los manifestantes porque los manifestantes temían la violencia.',\n",
       "  False))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = sentence_creation(data_un)\n",
    "# Example of the pairs of sentences\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f71cf1f",
   "metadata": {},
   "source": [
    "## Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97b095c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "batch_size = 8\n",
    "learning_rate = 0.001\n",
    "embedding_size = 256\n",
    "hidden_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aebca317",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_len, embedding_size, hidden_size):\n",
    "\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.vocab_len = vocab_len\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_len, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size//2)\n",
    "        self.fc2 = nn.Linear(hidden_size//2, vocab_len)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        embeds = self.embedding(x)\n",
    "        out, hidden = self.lstm(embeds)\n",
    "        fc1_out = self.fc1(out)\n",
    "        fc2_out = self.fc2(fc1_out)\n",
    "        output = self.softmax(fc2_out)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5d9f70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training function\n",
    "\n",
    "def train(path, epochs, batch_size, learning_rate, embedding_size, hidden_size, device):\n",
    "    \n",
    "    # loading the data\n",
    "    dataset, vocab = get_data(path)\n",
    "    \n",
    "    model = LSTM(len(vocab.vocab), embedding_size, hidden_size)\n",
    "    model.to(device)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # start training loop\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        for i, batch in enumerate(dataset):\n",
    "\n",
    "            # the strucure for each BATCH is:\n",
    "            # <start>, w0, ..., wn, <end>\n",
    "            sentence = batch.sentence\n",
    "\n",
    "            # we do not want to give <end> as input to the model\n",
    "            input_sentence = sentence[:, :-1]\n",
    "\n",
    "            # send to model\n",
    "            output = model(input_sentence)\n",
    "\n",
    "            # we select all but the first token from sentences\n",
    "            target = sentence[:, 1:]\n",
    "            \n",
    "            # loss\n",
    "            loss = loss_fn(output.permute(0, 2, 1), target)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # print average loss for the epoch\n",
    "            print(total_loss/(i+1), end='\\r') \n",
    "\n",
    "            # compute gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # reset gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        print()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91d02bf",
   "metadata": {},
   "source": [
    "#### Model created with wikipedia dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c216d97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.4633059020423895\n",
      "14.158298510017396\n",
      "20.546251615085602\n",
      "26.662729534435277\n",
      "32.530420682411295\n"
     ]
    }
   ],
   "source": [
    "# with medium_wiki_sent.csv\n",
    "model_medium = train('medium_wiki_sent.csv', epochs, batch_size, learning_rate, embedding_size, hidden_size, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "583586f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model\n",
    "torch.save(model_medium, 'lstm_model_medium.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14b36c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_medium = torch.load('lstm_model_medium.pt').to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8fc707",
   "metadata": {},
   "source": [
    "#### Model created with book dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b02929d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.4445716465479365\n",
      "10.270369262541992\n",
      "14.855445499952924\n",
      "19.255806524329685\n",
      "23.495890311415664\n"
     ]
    }
   ],
   "source": [
    "# with new_dataset\n",
    "model_book = train('sentences.csv', epochs, batch_size, learning_rate, embedding_size, hidden_size, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6dd28dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model\n",
    "torch.save(model_book, 'lstm_model_book.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2247dc5",
   "metadata": {},
   "source": [
    "## Calculating sentence probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "519c053d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getting_prob(sentences, vocab, model):\n",
    "    probs = []\n",
    "    n = 0\n",
    "    tot = 0\n",
    "    \n",
    "    for sent in sentences: # ((Sent1, True), (Sent2, False))\n",
    "        tot += 1\n",
    "        for pair in sent:\n",
    "            if pair[1]:\n",
    "                true_sent = pair[0]\n",
    "            else:\n",
    "                false_sent = pair[0]\n",
    "\n",
    "        # tokenizing sent\n",
    "        tok_true_sent = true_sent.split()\n",
    "        tok_false_sent = false_sent.split()\n",
    "\n",
    "        # encoding\n",
    "        enc_true_sent = torch.tensor([vocab.vocab.stoi[x] for x in tok_true_sent], device=device)\n",
    "        enc_false_sent = torch.tensor([vocab.vocab.stoi[x] for x in tok_false_sent], device=device)\n",
    "\n",
    "        # model\n",
    "        out_true_sent = model(enc_true_sent.unsqueeze(0))\n",
    "        out_false_sent = model(enc_false_sent.unsqueeze(0))\n",
    "\n",
    "        # get probabilities \n",
    "        true_prob = F.softmax(out_true_sent, dim=2)\n",
    "        false_prob = F.softmax(out_false_sent, dim=2)\n",
    "\n",
    "        # append\n",
    "        true_max = torch.max(true_prob, dim=2)\n",
    "        false_max = torch.max(false_prob, dim=2)\n",
    "        \n",
    "        true_sent_prob = torch.prod(true_max.values) # sum?\n",
    "        false_sent_prob = torch.prod(false_max.values)\n",
    "        \n",
    "        probs.append((true_sent_prob, false_sent_prob))\n",
    "        n += int(true_sent_prob > false_sent_prob)\n",
    "    \n",
    "    print('Accuracy:', n/tot)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0796d14c",
   "metadata": {},
   "source": [
    "#### Model created with wikipedia dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5f3e0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_wiki, vocab_wiki = get_data('medium_wiki_sent.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b53c89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4074074074074074\n"
     ]
    }
   ],
   "source": [
    "probs_wiki = getting_prob(sentences, vocab_wiki, model_medium)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641cc776",
   "metadata": {},
   "source": [
    "#### Model created with book dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f39c881",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_book, vocab_book = get_data('sentences.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cef36428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.44074074074074077\n"
     ]
    }
   ],
   "source": [
    "probs_book = getting_prob(sentences, vocab_book, model_book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb86f9fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
